<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en"><head><title>ML Reading List</title>

<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<style type="text/css">
body { font-size: 12px; font-family: Arial, sans-serif; }
dt { margin-top: 1em; font-weight: bold; }
</style></head>
<body><h1>Reading List</h1>

<h3 style="text-align: right;"><a href="index.html">Machine
Learning</a><br>
2009-I</h3>
<hr style="width: 100%; height: 2px; margin-left: auto; margin-right: 0px;">
<h3>Bayesian decision theory</h3>
<dl><dt>[Goldenberg05]</dt>
<dd>Goldenberg, A. &amp; Moore, A.</dd>
<dd><a href="http://www.cs.cmu.edu/%7Eanya/papers/link-kdd05.pdf">Bayes Net Graphs to Understand Coauthorship Networks</a></dd>

<dd><em>KDD Workshop on Link Discovery: Issues,
Approaches and Applications,</em><b> </b>2005</dd></dl><dl><dt style="font-weight: bold;">[MacKay98]&nbsp;</dt><dd>D.J.C. MacKay</dd><dd><a href="http://www.cse.ucsd.edu/classes/fa04/cse252c/monte.ps">Introduction to monte carlo methods</a></dd><dd style="font-style: italic;">Learning in graphical models, Kluwer, 1998, pp. 175&#8211;204.</dd></dl><dl><dt style="font-weight: bold;">[Myers99]&nbsp;</dt><dd>J.W. Myers, K.B. Laskey, and T.S. Levitt</dd><dd><a href="http://www.ite.gmu.edu/%7Eklaskey/papers/myersUAI99.pdf">Learning Bayesian networks from incomplete data with stochastic search algorithms</a></dd><dd style="font-style: italic;">Proceedings
of the Fifteenth Conference on Uncertainty in Artificial Intelligence,
Morgan Kaufmann Publishers, 1999, pp. 476-485.</dd><dt>[Rabiner89]</dt><dd>L.R. Rabiner&nbsp;</dd><dd><a href="http://www.cs.ubc.ca/%7Emurphyk/Bayes/rabiner.pdf">A tutorial on hidden Markov models and selected applications in speech recognition</a></dd><dd style="font-style: italic;">Proceedings of the IEEE,&nbsp; vol. 77, 1989, pp. 257-286.</dd></dl><dl><dt>[Tenenbaum06]</dt><dd>Tenenbaum, J. B.; Griffiths, T. L. &amp; Kemp, C.</dd><dd><a href="http://web.mit.edu/cocosci/Papers/tics-theories-reprint.pdf">Theory-based Bayesian models of inductive learning and
reasoning</a></dd><dd><em>Trends in Cognitive Sciences,</em></dd><dd><b>2006</b><i>, 10</i>, 309-318</dd><dt>[Yesidia03]&nbsp;</dt><dd>J.S. Yedidia, W.T. Freeman, and Y. Weiss</dd><dd><a href="http://www.cs.huji.ac.il/course/2005/pmai/tirguls/TR2001-22.pdf">Understanding belief propagation and its generalizations</a></dd><dd style="font-style: italic;">Exploring artificial intelligence in the new millennium, Morgan Kaufmann, 2003, pp. 239&#8211;236.</dd></dl><h3>Kernel methods</h3><dl><dt>[Borgwardt05]</dt><dd>&nbsp;K.M. Borgwardt, C.S. Ong, S. Schonauer, S.V.N. Vishwanathan, A.J. Smola, and H.P. Kriegel,&nbsp;</dd><dd><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.8974&amp;rep=rep1&amp;type=pdf">Protein function prediction via graph kernels</a></dd><dd>Bioinformatics,&nbsp; vol. 21, 2005, pp. i47-i56.</dd></dl><h3>Support Vector Learning</h3><dl><dt>[Hsu02]&nbsp;</dt><dd>C.W. Hsu and C.J. Lin</dd><dd><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.129.6281&amp;rep=rep1&amp;type=pdf">A comparison of methods for multiclass support vector machines</a></dd><dd>IEEE Transactions on Neural Networks,&nbsp; vol. 13, 2002, pp. 415-425.</dd></dl><dl><dt>[Smola04]&nbsp;</dt><dd>A.J. Smola and B. Schölkopf</dd><dd><a href="http://www.ra.cs.uni-tuebingen.de/lehre/ss04/NNFolien/SVRTutorial.pdf">A tutorial on support vector regression</a></dd><dd>Statistics and Computing,&nbsp; vol. 14, 2004, pp. 199-222.</dd></dl><dl><dt>[Tong01]&nbsp;</dt><dd>S. Tong and E. Chang</dd><dd><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.5409&amp;rep=rep1&amp;type=pdf">Support vector machine active learning for image retrieval</a></dd><dd>Proceedings of the ninth ACM international conference on Multimedia, ACM New York, NY, USA, 2001, pp. 107-118.</dd></dl><br><h3>Performance evaluation</h3><dl><dt>[Domingos99]</dt><dd>Domingos, P.&nbsp;</dd><dd><a href="http://elvis.slis.indiana.edu/irpub/KDD/1999/pdf19.pdf">MetaCost: a general method for making classifiers cost-sensitive.&nbsp;</a></dd><dd>In Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, p. 155-164</dd><dd> 1999</dd></dl><dl><dt>[Hand01]</dt><dd>Hand, David J., and Robert J. Till.&nbsp;</dd><dd><a href="http://dx.doi.org/10.1023/A:1010920819831">A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems.&nbsp;</a></dd><dd>Machine Learning 45, no. 2:171-186.</dd><dd> 2001</dd></dl><dl><dt>[Japkowicz02]</dt><dd>Japkowicz, N.&nbsp;</dd><dd><a href="http://www.site.uottawa.ca/%7Enat/Papers/ida-journal-final.ps.gz">The class imbalance problem: A systematic study.&nbsp;</a></dd><dd>Intelligent Data Analysis, 6(5), p.429-449.<br>2002</dd></dl><dl><dt>[Demsar06]&nbsp;</dt><dd>J. Dem&#353;ar</dd><dd><a href="http://ucilnica.fri.uni-lj.si/file.php/104/clanki/clus-2006-Demsar-Comparison_of_classifiers-JMLR.pdf">Statistical Comparisons of Classifiers over Multiple Data Sets</a></dd><dd>J. Mach. Learn. Res.,&nbsp; vol. 7, pp. 1-30</dd><dd>2006</dd></dl><h3>Combining multiple classifiers</h3><dl><dt>[Dietterich00]&nbsp;</dt><dd>Dietterich, T. G.&nbsp;</dd><dd><a href="http://140.109.73.167/file/Paper/Boosting/ref51.pdf">An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization.</a> Machine Learning 40, no. 2: 139-157.&nbsp;</dd><dd>2000<span style="font-weight: bold;"></span></dd></dl><dl><dt>[Mason00]&nbsp;</dt><dd>Mason, L., J. Baxter, P. Bartlett, and M. Frean.&nbsp;</dd><dd><a href="http://www.lsmason.com/papers/NIPS00-DOOMII.pdf">Boosting algorithms as gradient descent.&nbsp;</a></dd><dd>In Advances in Neural Information Processing Systems, 12:512-518.&nbsp;</dd><dd> 2000</dd></dl><dl><dt>[Oza05]</dt><dd>Oza, N.&nbsp;&nbsp;</dd><dd><a href="http://tc.arc.nasa.gov/people/oza/publications/files/oza05.pdf.">Online bagging and boosting.&nbsp;</a></dd><dd>In 2005 IEEE International Conference on Systems, Man and Cybernetics&nbsp;&nbsp;</dd><dd>2005</dd></dl><h3></h3><h3>Learning on complex-structured and non-structured data</h3><br><h3><br></h3>
</body></html>